---
layout: "step"
title: Subscriptions
nav_order: 7
permalink: /tutorial/subscriptions/
---

# Subscriptions

In this tutorial, we are going to learn how to enable subscriptions on our GraphQL server. Subscriptions allow our web apps to remain informed of various events
that are occurring in your environment as they occur. One of the most common subscriptions is to stay informed when data is changed.

Let's say we're creating a real-time dashboard of all of the books that are currently checked out of the library. In order to do that, we'd need to first make a
query to get all currently checked out books and display them on our dashboard. Updating the dashboard could involve making that same query again and updating
the results, but that would mean we will be transferring much of the same data that we already have when all we really need are the changes. We could create a
new query that we could pass a "since" date argument to and fetch a list of all of the changes, but we would have to do that on a timer in our client and if
there is a low volume of events where users are checking assets in and out of the library, that could result in a far greater number of requests to both our
GraphQL server & back-end data sources than is needed and our dashboard wouldn't really be "real-time". The most efficient way to create our real-time dashboard
is to allow our clients to be notified of a checkout or checkin event as it is happening via GraphQL subscriptions using web sockets.

## Scalability concerns

Since our GraphQL subscriptions will use web sockets to communicate updates to our clients, the client will need to keep a persistent connection to our GraphQL
server. As we scale our GraphQL deployments to multiple instances, we want to make certain that each of our instances keeps as little state as possible with the
goal being a stateless server. To help with this goal, we are going to abstract the events management to a system specifically designed for that purpose and
then have our GraphQL server hook into that events system for client handling. In this way, we can not only be certain that an event triggered by one instance
of our GraphQL server cluster can be received and acted upon by another GraphQL server, but also that our events are being handled in an efficient and scalable
manner that does not involve a lot of extra development and maintenance overhead.

The system that we are going to be using for this tutorial is [Apache Kafka](https://kafka.apache.org/). Kafka is fast, scalable, stable and may already be in
use at your organization. For this tutorial a [kafka
service](https://github.com/hydrateio/advanced-graphql-server-tutorial/blob/step-7-subscriptions/data-sources/docker-compose.yml#L39) and its dependant
[zookeeper service](https://github.com/hydrateio/advanced-graphql-server-tutorial/blob/step-7-subscriptions/data-sources/docker-compose.yml#L31) have been added
to our docker cluster. Before proceeding, make sure you have the updates in this branch and restart your local docker cluster with
<code class="language-shell">npm run data-stop && npm run data-start</code>. We will be implementing both Kafka Consumers and Producers. The Producers will be
responsible for sending messages to the Kafka system when an event occurs. The Consumers will be responsible for listening for events and passing them along to
the clients.

## Add Kafka Connections

### Install Dependencies

Just like MySQL and MongoDB, we'll need to choose a node library that we will use to connect to and interact with our Kafka cluster. For our project we are
going to use the [node-rdkafka](https://github.com/blizzard/node-rdkafka) library which is a wrapper around the C++
[librdkafka](https://github.com/edenhill/librdkafka) library. This library was chosen over other, possibly easier to install, options for performance, feature
set and support.  Since this node module will need to build the librdkafka library, there are some prerequisites that you may need to perform depending on your
system which are covered in the [Requirements section of the node-rdkafka README](https://github.com/blizzard/node-rdkafka#requirements). For recent versions of
macOS, this involves [installing openssl with homebrew](https://brewformulas.org/openssl) and [setting some environment
variables](https://github.com/blizzard/node-rdkafka#mac-os-high-sierra--mojave) before running <code class="language-shell">npm install</code>.

Kafka handles message subscriptions based on a group ID sent when creating a consumer instance. Since we're going to want multiple users (consumers) to receive
the same messages from a provider when it is published, each subscription (websocket) instance is going to need its own group ID. We're also going to be
installing the [uuid](https://github.com/kelektiv/node-uuid) library to create unique group IDs for each websocket connection made to be used when setting up
our Kafka Consumers.

### Create environment variables

For our basic Kafka connection to a local Kafka server running in Docker, we'll only need to reference a single environment variable which contains the list of
Kafka brokers that make up our Kafka cluster that we are going connect to. In our ".env" file we're going to add that variable.

<pre><code class="language-shell">
# File: .env
KAFKA_BROKERS=localhost:9092
</code></pre>

Then we handle that in our env.js file

<pre><code class="language-javascript">
// File: src/env.js
const environmentVariables = {
  ...
  <mark>KAFKA_BROKERS: process.env.KAFKA_BROKERS || null,</mark>
};
</code></pre>

### Create AsyncIterators for Apollo Server

Apollo Server implements GraphQL subscriptions using [Async Iterators](https://github.com/tc39/proposal-async-iteration). Basically, an Async Iterator is an
object with a `next()` method that when called returns a Promise that will eventually resolve with an object containing both a <code
class="language-javascript">value</code> and <code class="language-javascript">done</code> key. If the value of <code class="language-javascript">done</code> is
false, the consumer knows there are more items to fetch and calls <code class="language-javascript">.next()</code> again.

To use Async Iterators with Kafka, we'll need to create a small wrapper around the data events of our Kafka consumer to resolve the <code
class="language-javascript">.next()</code> methods of our async iterator with the message from the data event. That way, when our Apollo Server subscription
calls <code class="language-javascript">.next()</code> it will wait for the data event from the Kafka server before resolving.

This wrapper is best broken up into two pieces: a generic utility to convert emitted events to async iterators and a library used to handle connections,
subscriptions and events for our Kafka server.

The first part is going to be based off of the
[event-emitter-to-async-iterator](https://github.com/apollographql/graphql-subscriptions/blob/master/src/event-emitter-to-async-iterator.ts) file of the
[graphql-subscriptions](https://github.com/apollographql/graphql-subscriptions) library. One change we will want to make here is to pass the handling of adding and
removing multiple topics to the kafka handler since kafka subscriptions expect arrays. This will save a few calls to our librdkafka library. The other change is
to add some error handling that will allow us to terminate the async iterator and send an error back to the client. We'll do this in order to inform the client
if there is ever a problem connecting to Kafka.

<pre><code class="language-javascript">
// File: src/data-connectors/event-emitter-to-async-iterator.js
import { $$asyncIterator } from 'iterall';

/**
 * Based on https://github.com/apollographql/graphql-subscriptions/blob/master/src/event-emitter-to-async-iterator.ts
 * Change made to addEventListeners and removeEventListeners to pass handling of multiple events to pubsub engine.
 * throw() method updated to accept and have emptyQueue function handle errors from the PubSub engine
 */
export function eventEmitterAsyncIterator(eventEmitter, eventsNames) {
  const pullQueue = [];
  const pushQueue = [];
  const eventsArray = typeof eventsNames === 'string' ? [eventsNames] : eventsNames;
  let listening = true;
  let addedListeners = false;

  const pushValue = (event) => {
    if (pullQueue.length !== 0) {
      pullQueue.shift().resolve({ value: event, done: false });
    } else {
      pushQueue.push(event);
    }
  };

  const pullValue = () => new Promise((resolve, reject) => {
    if (pushQueue.length !== 0) {
      resolve({ value: pushQueue.shift(), done: false });
    } else {
      pullQueue.push({ resolve, reject });
    }
  });

  const addEventListeners = () => {
    eventEmitter.addListener(eventsArray, pushValue);
  };

  const removeEventListeners = () => {
    eventEmitter.removeListener(eventsArray, pushValue);
  };

  const emptyQueue = (error) => {
    if (listening) {
      listening = false;
      if (addedListeners) { removeEventListeners(); }
      if (error) {
        const { message, extensions } = error;
        pullQueue.forEach(promise => promise.reject({ message, extensions }));
      } else {
        pullQueue.forEach(promise => promise.resolve({ value: undefined, done: true }));
      }
      pullQueue.length = 0;
      pushQueue.length = 0;
    }
  };

  return {
    next() {
      if (!listening) { return this.return(); }
      if (!addedListeners) {
        addEventListeners();
        addedListeners = true;
      }
      return pullValue();
    },
    return() {
      emptyQueue();
      return Promise.resolve({ value: undefined, done: true });
    },
    throw(error) {
      emptyQueue(error);
    },
    [$$asyncIterator]() {
      return this;
    },
  };
}
</code></pre>

Now we create the file to handle our Kafka connections, consumer and producers. Since each websocket connection is going to need its own consumer with a unique
group ID, we'll implement our Kafka connector as a class that can be instantiated on each websocket connection. The node-rdkakfa library is not a promise based
library, but we can do a little work to handle connections with promises and then use async/await to make sure that we're not trying to listen to events before
a connection is ready. That functionality is implemented in the <code class="language-javascript">getProducer()</code>, <code
class="language-javascript">getConsumer()</code>, and <code class="language-javascript">waitFor()</code> methods of our class.

If there are any issues connecting to our Kafka brokers, we'll handle those errors by logging them to the console. Since our Subscriptions rely on a connection
to the Kafka brokers, if a connection cannot be created we will throw that error to the instance of the async iterator so that it can pass it along to the
client to be informed that subscriptions are currently unavailable.

The <code class="language-javascript">asyncIterator()</code> method is going to be called by each of our GraphQL Subscription resolvers and will return a new
Async Iterator which listens to each of the topics passed. The <code class="language-javascript">value</code> part of the resolved data of our Async Iterator's
<code class="language-javascript">.next()</code> call will need to be an object containing a single key which is the name of the subscription method and holds
an object matching the response specified in the Subscription typeDefs. For this reason, our asyncIterator method will accept both a subscriptionName along with
an array of all of the topics that our subscription is listening for.  Our <code class="language-javascript">addListener()</code> and <code
class="language-javascript">removeListener()</code> methods are called by our <code class="language-javascript">eventEmitterAsyncIterator()</code> when new
subscriptions are made and destroyed. Finally our <code class="language-javascript">publish()</code> method is called during the handling of various events such
as query or manipulation handling.

<pre><code class="language-javascript">
// File: src/data-connectors/kafka.js
import * as Kafka from 'node-rdkafka';
import { v4 as uuid } from 'uuid';
import { ApolloError } from 'apollo-server-express';
import env from '../env';
import { eventEmitterAsyncIterator } from './event-emitter-to-async-iterator';

const serviceError = new ApolloError('Subscription Service Unavailable', 'SUBSCRIPTION_SYSTEM_ERROR');

export class KafkaPubSub {
  constructor() {
    this.kafkaGlobalConfig = {
      'group.id': uuid(),
      'metadata.broker.list': env.KAFKA_BROKERS,
    };
    this.iterator = null;
    this.subscriptionName = null;
    this.subscribedTopics = [];
    this.producer = null;
    this.producerIsReady = false;
    this.consumer = null;
    this.consumerIsReady = false;
  }

  async publish(topic, message) {
    await this.getProducer();
    try {
      this.producer.produce(
        topic,
        null,
        Buffer.from(JSON.stringify(message)),
      );
    } catch (e) {
      console.error('There was an error sending message');
      console.error(e);
    }
  }

  async addListener(topics, onMessage) {
    await this.getConsumer();
    let wasTopicAdded = false;
    topics.forEach((topic) => {
      if (!this.subscribedTopics.includes(topic)) {
        this.subscribedTopics.push(topic);
        wasTopicAdded = true;
      }
    });
    if (wasTopicAdded) {
      this.consumer.unsubscribe();
      this.consumer.subscribe(this.subscribedTopics);
      this.consumer.consume();
    }
    this.consumer.on('data', (data) => {
      const message = JSON.parse(data.value.toString());
      onMessage({ [this.subscriptionName]: message });
    });
  }

  async removeListener(topics) {
    await this.getConsumer();
    let wasTopicRemoved = false;
    this.subscribedTopics = this.subscribedTopics.filter((topic) => {
      wasTopicRemoved = true;
      return !topics.includes(topic);
    });
    if (wasTopicRemoved) {
      this.consumer.unsubscribe();
      if (this.subscribedTopics.length > 0) {
        this.consumer.subscribe(this.subscribedTopics);
        this.consumer.consume();
      }
    }
  }

  asyncIterator(subscriptionName, topics) {
    this.subscriptionName = subscriptionName;
    this.iterator = eventEmitterAsyncIterator(this, topics);
    return this.iterator;
  }

  async getProducer() {
    if (!this.producer) {
      this.producer = new Kafka.Producer(this.kafkaGlobalConfig);
      this.producer.connect((err) => {
        if (err) {
          console.error('connection error:', err);
        }
      });
      this.producer.on('ready', () => {
        this.producerIsReady = true;
      });
      this.producer.on('event.error', (error) => {
        console.error('producer event error', error);
      });
      this.producer.on('connection.failure', (err) => {
        console.error('producer connection failure:', err);
      });
    }
    await this.waitFor('producer');
    return this.producer;
  }

  async getConsumer() {
    if (!this.consumer) {
      this.consumer = new Kafka.KafkaConsumer(this.kafkaGlobalConfig);
      this.consumer.connect((err) => {
        if (err) {
          console.error('connection error:', err);
          this.iterator.throw(serviceError);
        }
      });
      this.consumer.on('ready', () => {
        this.consumerIsReady = true;
      });
      this.consumer.on('event.error', (error) => {
        console.error('consumer event error', error);
        this.iterator.throw(serviceError);
      });
      this.consumer.on('connection.failure', (err) => {
        console.error('consumer connection failure:', err);
        this.iterator.throw(serviceError);
      });
    }
    await this.waitFor('consumer');
    return this.consumer;
  }

  waitFor(item) {
    return new Promise((resolve) => {
      const checkConnected = () => {
        if (this[`${item}IsReady`]) {
          resolve();
        } else {
          setTimeout(() => checkConnected(), 100);
        }
      };
      checkConnected();
    });
  }
}
</code></pre>

> If you've gotten to this point and have been incrementing your code from the start of the tutorial, the rest of the kafka setup can be added by cherry picking these commits [06e77f](https://github.com/hydrateio/advanced-graphql-server-tutorial/commit/06e77f836aef0e8878d367b3a45275dc33561f3f) & [f1254f4](https://github.com/hydrateio/advanced-graphql-server-tutorial/commit/f1254f40a51a430850e5495d29980cf3c91c1e87).

## Enable subscription support on server

Now, we must enable websocket support on our GraphQL server. This step is fairly simple since we are using apollo-server-express for
our server. Our ApolloServer instance that we've created includes an <code class="language-javascript">installSubscriptionsHandlers()</code> method which takes
care of most of the web socket set up for us. This method takes an instance of a nodejs [http.Server](https://nodejs.org/api/http.html#http_class_http_server),
so we'll first import the http library and create an http.Server instance by passing our express app to the <code
class="language-javascript">http.createServer()</code> method. Now we can use this instance to install the subscription handlers before we start listening for
connections.

<pre><code class="language-javascript">
// File: src/index.js
import * as http from 'http';
import express from 'express';
import { ApolloServer } from 'apollo-server-express';
import schema from './schema';
import context from './context';
import env from './env';

const app = express();
const server = new ApolloServer({
  schema,
  context,
});

server.applyMiddleware({ app });
const httpServer = http.createServer(app);
server.installSubscriptionHandlers(httpServer);

// eslint-disable-next-line no-console
httpServer.listen({ port: env.GRAPHQL_SERVER_PORT }, () => console.log(`ðŸš€ Server ready at http://localhost:${env.GRAPHQL_SERVER_PORT}${server.graphqlPath}`));
</code></pre>

Now we need to make a small update to our context creator. WebSockets maintain a persistent connection to our server that was made during initial handshakes.
Our context handlers are going to need to check to see if that connection has already been made and then extend the current context if it has. This way we won't
be overwriting any context that may have been added during initial websocket handshakes. We'll also be extending our context and adding our Kafka unique pubsub
instance to make it available to our subscribers and our data resolvers.

<pre><code class="language-javascript">
// File: src/context/index.js
import getDataLoaders from './data-loaders';
import { KafkaPubSub } from '../data-connectors/kafka';

export default async ({ connection }) => {
  const kafka = new KafkaPubSub();
  if (connection) {
    return {
      ...connection.context,
      loaders: getDataLoaders(),
      pubsub: { kafka },
    };
  }
  return {
    loaders: getDataLoaders(),
    pubsub: { kafka },
  };
};
</code></pre>

## Update CheckOut schema

Finally we will need to have a way to subscribe to and publish events to our Kafka server so we'll need to add the Subscriptions to our typeDefs and properly
handle the events. In order to create our live-feed checkout dashboard we'd need to be informed when assets are both checked out of and back in to the library.
In our typeDef we'll specify these events by extending the Subscription type. Here we can add one subscription for checkins, one for checkouts and another
that will watch for messages regarding either the checkin or checkout topic.

<pre><code class="language-graphql">
// File: src/schema/checkout/checkout.typeDef.js
const typeDef = /* GraphQL */`
  ...
  extend type Subscription {
    checkoutStatusUpdate: CheckOut
    checkout: CheckOut
    checkin: CheckOut
  }
  ...
}
</code></pre>

To implement our subscriptions in our resolvers we are going to need to make a call to the kafka pubsub instance that we had earlier attached to our context when
initiating a new subscriptions. When calling the <code class="language-javascript">asyncIterator()</code> method, we'll need to pass the name of our
subscription so that the <code class="language-javascript">value</code> of the object resolved by our iterator <code class="language-javascript">next()</code>
method is keyed correctly. Next we'll pass the topics we want to subscribe to. You'll notice that our `checkoutStatusUpdate` subscription will listen for both
`checkin` and `checkout` topics.

<pre><code class="language-javascript">
// File: src/schema/checkout/checkout.resolvers.js
export default {
  ...
  Subscription: {
    checkoutStatusUpdate: {
      subscribe: (root, args, context) => context.pubsub.kafka.asyncIterator('checkoutStatusUpdate', ['checkin', 'checkout']),
    },
    checkout: {
      subscribe: (root, args, context) => context.pubsub.kafka.asyncIterator('checkout', ['checkout']),
    },
    checkin: {
      subscribe: (root, args, context) => context.pubsub.kafka.asyncIterator('checkin', ['checkin']),
    },
  },
  ...
}
</code></pre>

To complete the setup, we're going to need to send a message to the Kafka server when either a checkin or checkout event occurs. We'll do that in our mutation
handlers. Our <code class="language-javascript">checkoutAsset()</code> function handles the <code class="language-javascript">checkoutAsset()</code> mutation,
so we'll update it to read the context argument that is passed to it and contains the kafka pubsub instance. By calling our <code
class="language-javascript">publish()</code> method on that instance, we are sending the same results that are sent to the client making the mutation to the
kafka server and attaching it to the `checkout` topic.

<pre><code class="language-javascript">
// File: src/schema/context/checkout.model.js
export async function checkoutAsset(root, args, <mark>context</mark>) {
  ...
  if (insertResults.affectedRows === 1) {
    const newRowQuery = mysqlDataConnector.format(`SELECT ${mappedQueryFields} FROM checkouts WHERE asset_upc=? AND checkin_date IS NULL`, args.assetUpc);
    const queryResults = await mysqlDataConnector.pool.query(newRowQuery);
    <mark>context.pubsub.kafka.publish('checkout', queryResults[0]);</mark>
    return queryResults[0];
  }
  ...
}
</code></pre>

We'll update our `checkinAsset` mutation handler in the exact same manner.

<pre><code class="language-javascript">
// File: src/schema/context/checkout.model.js
export async function checkinAsset(root, args, <mark>context</mark>) {
  ...
  const results = await mysqlDataConnector.pool.query(verifyQuery);
  <mark>context.pubsub.kafka.publish('checkin', results[0]);</mark>
  return results[0];
}
</code></pre>

## Verify

We can check to make certain everything is now working by once again going to our GraphQL Playground at http://localhost:4000/graphql and creating some new
subscriptions.  The GraphQL Playground has a tab interface, so create a new tab for each of the subscriptions below

<pre><code class="language-graphql">
subscription {
  checkin {
    id
    assetUpc
    checkinDate
    checkoutDate
    patron {
      email
      firstName
      lastName
    }
  }
}
</code></pre>

<pre><code class="language-graphql">
subscription {
  checkout {
    id
    assetUpc
    checkinDate
    checkoutDate
    patron {
      email
      firstName
      lastName
    }
  }
}
</code></pre>

<pre><code class="language-graphql">
subscription {
  checkoutStatusUpdate {
    id
    assetUpc
    checkinDate
    checkoutDate
    patron {
      email
      firstName
      lastName
    }
  }
}
</code></pre>

With the subscriptions running, send a checkout mutation.

<pre><code class="language-graphql">
mutation {
  checkoutAsset(assetUpc: "9000000002", userEmail:"smattschas7@discuz.net") {
    id
    userEmail
    assetUpc
    checkinDate
    checkoutDate
  }
}
</code></pre>

> Note: it's possible that the book may already be checked out when the checkoutAsset mutation is run. Simply skip ahead and run the checkinAsset mutation below and then re-run the code above.

Now switch over to both the tab for the checkout and checkoutStatusUpdate subscriptions, you should see the checkout object that matches the response from the
mutations.  Since we are not listing for the checkout topic on our checkin subscription, if you switch to the checkin subscription tab, you can verify that it
did not receive any messages.

Note: The first time any particular topic is sent to the a Kafka server, the server cluster must do some initial setup tasks for that topic if the topic was not
created when the server started up. You may notice that your very first mutation takes a short time to be received by the consumers.

We'll also send a checkin mutation to verify everything is working the other way around.

<pre><code class="language-graphql">
mutation {
  checkinAsset(assetUpc: "9000000002") {
    id
    userEmail
    assetUpc
    checkinDate
    checkoutDate
  }
}
</code></pre>

Checking our subscription tabs, we'll notice an event was sent to both the checkin and checkoutStatusUpdate subscriptions, but no the checkout subscription.
